{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3612fb",
   "metadata": {},
   "source": [
    "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b225c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hadoop_config(file_path):\n",
    "    core_components = []\n",
    "    with open(file_path, 'r') as config_file:\n",
    "        for line in config_file:\n",
    "            if line.startswith(\"fs.defaultFS\"):\n",
    "                core_components.append(\"NameNode\")\n",
    "            elif line.startswith(\"yarn.resourcemanager.hostname\"):\n",
    "                core_components.append(\"ResourceManager\")\n",
    "            elif line.startswith(\"mapreduce.jobhistory.address\"):\n",
    "                core_components.append(\"JobHistoryServer\")\n",
    "    return core_components\n",
    "\n",
    "# Example usage\n",
    "config_file_path = \"/path/to/hadoop/conf/hadoop-site.xml\"\n",
    "components = read_hadoop_config(config_file_path)\n",
    "print(\"Core Components of Hadoop:\")\n",
    "for component in components:\n",
    "    print(component)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d8fa2",
   "metadata": {},
   "source": [
    "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def calculate_directory_size(directory_path):\n",
    "    command = f\"hadoop fs -du -s {directory_path}\"\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        output = result.stdout.strip()\n",
    "        size_in_bytes = int(output.split()[0])\n",
    "        return size_in_bytes\n",
    "    else:\n",
    "        print(\"Error occurred while calculating directory size.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "hdfs_directory = \"/user/data\"\n",
    "total_size = calculate_directory_size(hdfs_directory)\n",
    "print(f\"Total size of {hdfs_directory}: {total_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309bd07",
   "metadata": {},
   "source": [
    "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f36605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def process_large_text_file(file_path, top_n):\n",
    "    word_counts = Counter()\n",
    "    with open(file_path, 'r') as text_file:\n",
    "        for line in text_file:\n",
    "            words = line.strip().split()\n",
    "            word_counts.update(words)\n",
    "    top_words = word_counts.most_common(top_n)\n",
    "    return top_words\n",
    "\n",
    "# Example usage\n",
    "text_file_path = \"/path/to/large_text_file.txt\"\n",
    "top_n = 10\n",
    "top_words = process_large_text_file(text_file_path, top_n)\n",
    "print(f\"Top {top_n} most frequent words:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de663f63",
   "metadata": {},
   "source": [
    "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_hadoop_cluster_health():\n",
    "    namenode_url = \"http://namenode:50070/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
    "    datanode_url = \"http://datanode:50075/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "\n",
    "    response_namenode = requests.get(namenode_url)\n",
    "    response_datanode = requests.get(datanode_url)\n",
    "\n",
    "    if response_namenode.status_code == 200 and response_datanode.status_code == 200:\n",
    "        namenode_status = response_namenode.json()['beans'][0]['State']\n",
    "        datanode_status = response_datanode.json()['beans'][0]['State']\n",
    "        print(f\"NameNode status: {namenode_status}\")\n",
    "        print(f\"DataNode status: {datanode_status}\")\n",
    "    else:\n",
    "        print(\"Error occurred while checking Hadoop cluster health.\")\n",
    "\n",
    "# Example usage\n",
    "check_hadoop_cluster_health()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3686ea",
   "metadata": {},
   "source": [
    "5. Develop a Python program that lists all the files and directories in a specific HDFS path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def list_hdfs_files_and_directories(path):\n",
    "    command = f\"hadoop fs -ls {path}\"\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        output = result.stdout.strip().split('\\n')\n",
    "        file_paths = [line.split()[-1] for line in output[1:]]  # Ignore the first line (total count)\n",
    "        return file_paths\n",
    "    else:\n",
    "        print(\"Error occurred while listing files and directories.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "hdfs_path = \"/user/data\"\n",
    "file_paths = list_hdfs_files_and_directories(hdfs_path)\n",
    "print(f\"Files and directories in {hdfs_path}:\")\n",
    "for path in file_paths:\n",
    "    print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678f3eb",
   "metadata": {},
   "source": [
    "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_datanode_storage_utilization():\n",
    "    datanode_url = \"http://datanode:50075/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState\"\n",
    "\n",
    "    response_datanode = requests.get(datanode_url)\n",
    "\n",
    "    if response_datanode.status_code == 200:\n",
    "        datanode_stats = response_datanode.json()['beans'][0]\n",
    "        storage_utilizations = {stats['name']: stats['used'] / stats['capacity'] for stats in datanode_stats['Storage']]\n",
    "        sorted_utilizations = sorted(storage_utilizations.items(), key=lambda x: x[1])\n",
    "        highest_utilization_node, highest_utilization = sorted_utilizations[-1]\n",
    "        lowest_utilization_node, lowest_utilization = sorted_utilizations[0]\n",
    "        print(\"DataNode Storage Utilization:\")\n",
    "        for node, utilization in sorted_utilizations:\n",
    "            print(f\"{node}: {utilization:.2%}\")\n",
    "        print(f\"\\nHighest Utilization: {highest_utilization_node} ({highest_utilization:.2%})\")\n",
    "        print(f\"Lowest Utilization: {lowest_utilization_node} ({lowest_utilization:.2%})\")\n",
    "    else:\n",
    "        print(\"Error occurred while analyzing DataNode storage utilization.\")\n",
    "\n",
    "# Example usage\n",
    "analyze_datanode_storage_utilization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664e21f",
   "metadata": {},
   "source": [
    "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9917ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(jar_path, input_path, output_path):\n",
    "    submit_url = \"http://resourcemanager:8088/ws/v1/cluster/apps/new-application\"\n",
    "    response = requests.post(submit_url)\n",
    "    if response.status_code == 200:\n",
    "        application_id = response.json()['application-id']\n",
    "        submit_job_url = f\"http://resourcemanager:8088/ws/v1/cluster/apps/{application_id}/app\"\n",
    "        data = {\n",
    "            \"application-id\": application_id,\n",
    "            \"application-name\": \"Hadoop Job\",\n",
    "            \"am-container-spec\": {\n",
    "                \"commands\": {\n",
    "                    \"command\": f\"hadoop jar {jar_path} {input_path} {output_path}\"\n",
    "                },\n",
    "                \"local-resources\": {\n",
    "                    \"entry\": [\n",
    "                        {\n",
    "                            \"key\": \"AppMaster.jar\",\n",
    "                            \"value\": {\n",
    "                                \"resource\": \"file:/path/to/AppMaster.jar\",\n",
    "                                \"type\": \"FILE\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        response = requests.post(submit_job_url, json=data)\n",
    "        if response.status_code == 202:\n",
    "            print(f\"Hadoop job submitted. Application ID: {application_id}\")\n",
    "            return application_id\n",
    "        else:\n",
    "            print(\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caefaa4b",
   "metadata": {},
   "source": [
    "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cfc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(jar_path, input_path, output_path, memory, vcores):\n",
    "    submit_url = \"http://resourcemanager:8088/ws/v1/cluster/apps/new-application\"\n",
    "    response = requests.post(submit_url)\n",
    "    if response.status_code == 200:\n",
    "        application_id = response.json()['application-id']\n",
    "        submit_job_url = f\"http://resourcemanager:8088/ws/v1/cluster/apps/{application_id}/app\"\n",
    "        data = {\n",
    "            \"application-id\": application_id,\n",
    "            \"application-name\": \"Hadoop Job\",\n",
    "            \"am-container-spec\": {\n",
    "                \"commands\": {\n",
    "                    \"command\": f\"hadoop jar {jar_path} {input_path} {output_path}\"\n",
    "                },\n",
    "                \"local-resources\": {\n",
    "                    \"entry\": [\n",
    "                        {\n",
    "                            \"key\": \"AppMaster.jar\",\n",
    "                            \"value\": {\n",
    "                                \"resource\": \"file:/path/to/AppMaster.jar\",\n",
    "                                \"type\": \"FILE\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"resources\": {\n",
    "                    \"memory\": memory,\n",
    "                    \"vCores\": vcores\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        response = requests.post(submit_job_url, json=data)\n",
    "        if response.status_code == 202:\n",
    "            print(f\"Hadoop job submitted. Application ID: {application_id}\")\n",
    "            return application_id\n",
    "        else:\n",
    "            print(\"Error occurred while submitting Hadoop job.\")\n",
    "    else:\n",
    "        print(\"Error occurred while acquiring new application ID.\")\n",
    "\n",
    "def track_resource_usage(application_id):\n",
    "    while True:\n",
    "        app_url = f\"http://resourcemanager:8088/ws/v1/cluster/apps/{application_id}\"\n",
    "        response = requests.get(app_url)\n",
    "        if response.status_code == 200:\n",
    "            app_info = response.json()['app']\n",
    "            if app_info['state'] == 'RUNNING':\n",
    "                resources = app_info['allocatedResources']\n",
    "                memory = resources['memory']\n",
    "                vcores = resources['vCores']\n",
    "                print(f\"Resource usage - Memory: {memory}, vCores: {vcores}\")\n",
    "            elif app_info['state'] in ['FINISHED', 'FAILED', 'KILLED']:\n",
    "                print(\"Hadoop job execution completed.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Error occurred while tracking resource usage.\")\n",
    "        time.sleep(5)\n",
    "\n",
    "# Example usage\n",
    "jar_file_path = \"/path/to/MapReduceJob.jar\"\n",
    "input_file_path = \"/path/to/input\"\n",
    "output_file_path = \"/path/to/output\"\n",
    "memory_requirement = 4096\n",
    "vcores_requirement = 4\n",
    "\n",
    "application_id = submit_hadoop_job(jar_file_path, input_file_path, output_file_path, memory_requirement, vcores_requirement)\n",
    "if application_id:\n",
    "    track_resource_usage(application_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74699451",
   "metadata": {},
   "source": [
    "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718030c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_mapreduce_job(input_path, output_path, split_size):\n",
    "    command = f\"hadoop jar MapReduceJob.jar {input_path} {output_path} {split_size}\"\n",
    "    start_time = time.time()\n",
    "    subprocess.run(command, shell=True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return execution_time\n",
    "\n",
    "# Example usage\n",
    "input_file_path = \"/path/to/input\"\n",
    "output_file_path = \"/path/to/output\"\n",
    "split_sizes = [64, 128, 256]  # In MB\n",
    "\n",
    "for split_size in split_sizes:\n",
    "    execution_time = run_mapreduce_job(input_file_path, output_file_path, split_size)\n",
    "    print(f\"Input split size: {split_size} MB | Execution time: {execution_time} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
